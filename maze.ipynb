{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPoPRDvSvOqtv8kMJkuakVK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Homework 2 - Aleksa Pulai RA22/2020, Katarina Topolic RA164/2020\n","\n","The following is the solution for the Level 3 assignment for the Maze problem.\n","The objective was to create a Maze in a form of a graph.\n","\n"," For our implementation we have decided to do it the following way: you can choose how many nodes (states) the graph will have, and the number of different actions that are possible in the entire graph. The nodes are connected by randomly choosing the number of connections (up to the maximum number of connections provided as a parameter) for each node, and the action having to be taken to \"cross\" from one node to the other. Let's say we have chosen to have a graph with 10 nodes, and 4 actions in total, with the maximum number of connections for each node being 5. For examle, Node1 could be connected with 3 other randomly chosen nodes, with randomly chosen actions (e.g. Node3 - Action1, Node7 - Action4, Node1 - Action4; as you can see it is possible for a node to be connected with itself, of course). We could end up with some nodes being connected with only one node, and some with 5 connections, and the distribution of the actions totaly random as well. Additionaly, every connection is assigned a probability, coresponding to the probability of that particular connection being chosen (or, actually, the next state) if a specific action is chosen (using the previous example we could say that the probability of going to the Node3 from Node1 if the Action1 is chosen is 1.0, and if the chosen action is Action4 the probabilities of Node7 and Node1 being the next state is let's say 0.3 and 0.7 respecfuly) The distribution of probabilities is, also random. The type of nodes (Regular, Penalty, Teleport and Terminal) is specified, by saying how many nodes of which type we want.\n","\n","The graph is stored in form of a dictionary, the keys being IDs of the state (integers from 0 to n-1, with n = number of nodes), and the values are lists, the first element being the type of the node, and the second is another dictionary, the keys of which are actions possible to choose in that state and the values a list containing next states and the corresponding probabilities of that next state being chosen with the assigned action."],"metadata":{"id":"9x-g6tsdkgey"}},{"cell_type":"markdown","source":["## Imports\n"],"metadata":{"id":"wjbaelOmkxEh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2piD0-LTkfoG"},"outputs":[],"source":["from abc import ABC, abstractmethod\n","from typing import Iterable, Callable\n","from copy import copy\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import random"]},{"cell_type":"markdown","source":["## Implementation of individual Nodes"],"metadata":{"id":"QdEpGMI3k0Tr"}},{"cell_type":"code","source":["class Node(ABC):\n","    \"\"\"Abstract base class for all maze nodes.\"\"\"\n","\n","    @abstractmethod\n","    def get_reward(self) -> float:\n","        \"\"\"The reward an agent receives when stepping onto this node.\"\"\"\n","        pass\n","\n","    def is_terminal(self) -> bool:\n","        \"\"\"Checks if the node is terminal.\n","\n","        When stepping onto a terminal node the agent exits\n","        the maze and finishes the game.\n","        \"\"\"\n","        return False\n","\n","    def has_value(self) -> bool:\n","        \"\"\"Check if the node has value.\"\"\"\n","        return True"],"metadata":{"id":"U2Rmrj_Fk7JA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RegularNode(Node):\n","    \"\"\"A common, non-terminal, steppable node.\"\"\"\n","\n","    def __init__(self, reward: float):\n","        self.reward = reward\n","\n","    def get_reward(self) -> float:\n","        return self.reward\n","\n","class PenaltyNode(Node):\n","    \"\"\"A common, non-terminal, steppable node.\"\"\"\n","\n","    def __init__(self, reward: float):\n","        self.reward = reward\n","\n","    def get_reward(self) -> float:\n","        return self.reward\n","\n","class TerminalNode(Node):\n","    \"\"\"A terminal node.\"\"\"\n","\n","    def __init__(self, reward: float):\n","        self.reward = reward\n","\n","    def get_reward(self) -> float:\n","        return self.reward\n","\n","    def is_terminal(self) -> bool:\n","        return True\n","\n","    def has_value(self) -> bool:\n","        return False\n","\n","\n","class TeleportNode(Node):\n","    \"\"\"A non-steppable node.\"\"\"\n","\n","    def __init__(self, reward: float):\n","        self.reward = reward\n","\n","    def get_reward(self) -> float:\n","        return 0\n","\n","    def has_value(self) -> bool:\n","        return False"],"metadata":{"id":"pNb8BopKlBLQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Implementation of the Maze Graph"],"metadata":{"id":"9oI-jTPzl_zT"}},{"cell_type":"code","source":["class MazeGraph:\n","\n","    def get_random_state(self, S_num: int) -> int:\n","        return random.randint(0,S_num-1)\n","\n","    def get_random_action(self, A_num: int) -> int:\n","        return random.randint(0,A_num-1)\n","\n","    def asign_probability_to_list(self, size: int) -> list:\n","        \"\"\"\n","            Returns a list, whose elements are probabilities the sum of which is 1, and are rounded at two decimal points (e.g. [0.3, 0.35, 0.35]).\n","\n","            Sometimes the sum turns out to be 0.99 or 1.01 for some reason. The purpose of if statements is to fix this.\n","        \"\"\"\n","        probabilities = [round(random.uniform(0,1), 2) for _ in range(size)]\n","\n","        total_prob = sum(probabilities)\n","        normalized_probabilities = [round(prob / total_prob, 2) for prob in probabilities]\n","\n","        remainder = round(1.00 - sum(normalized_probabilities), 2)\n","\n","        if remainder == 0:\n","            return normalized_probabilities\n","        elif remainder > 0:\n","            i = 0\n","            while normalized_probabilities[i] < remainder:\n","                i += 1\n","            round(normalized_probabilities[i] + remainder, 2)\n","            return normalized_probabilities\n","        else:\n","            normalized_probabilities[0] = round(normalized_probabilities[0] + remainder, 2)\n","            return normalized_probabilities\n","\n","    def asign_prob_to_connections(self, connections: dict[int, list[int,float]]) -> dict[int, list[int,float]]:\n","        '''\n","          This method creates connections for the given state, and the states it is connected to, assigning actions and probabilities of each connection.\n","\n","          connections - {action_for_that_connection: [next_state, probability of that connection]\n","        '''\n","        for action in connections:\n","            probabilities = self.asign_probability_to_list(len(connections[action]))\n","            for i in range(0, len(connections[action])):\n","                connections[action][i][1] = probabilities[i]\n","        return connections\n","\n","    def connect_state(self, state_id: int, S_num: int, A_num: int, max_connections: int) -> dict[int, list[list[int,float]]]:\n","\n","        '''Makes a dictionary consisting of connections between state (ID = state_id) and other states.\n","\n","        state_id = the id of the state for which the connections are being made\n","        S_num = the total number of states in the graph\n","        A_num = the number of possible actions (total number of actions in the graph) to choose and get to the next state from the state (ID = state_id)\n","        max_connections = maximum number of connections with the state (ID = state_id)'''\n","        n = random.randint(1,max_connections)\n","        res = {}\n","        actions_help = {} #to ensure that we dont have two states connected by two connections with the same action\n","        for i in range(n):\n","            next_state = self.get_random_state(S_num)\n","            action = self.get_random_action(A_num)\n","            if action not in res:\n","                res[action] = [[next_state, -1]]\n","                actions_help[action] = {next_state}\n","            elif next_state not in actions_help[action]:\n","                res[action].append([next_state, -1])\n","                actions_help[action].add(next_state)\n","        res = {k: sorted(v, key=lambda x: x[0]) for k, v in sorted(res.items())} #sorts the resulting dictionary by actions\n","        return res\n","\n","    def connect_teleport(self, state_id: int, S_num: int) -> dict[int, list[list[int,float]]]:\n","        \"\"\"\n","            Defines the node to which the agent is going to be teleported to.\n","        \"\"\"\n","        next_state = self.get_random_state(S_num)\n","        while next_state == state_id: #to ensure that it doesn't teleport to the same node (infinite teleportation :( )\n","            next_state = self.get_random_state(S_num)\n","        return {0: [[next_state, 1.0]]}\n","\n","    def connect_terminal(self, state_id: int) -> dict[int, list[list[int,float]]]:\n","        return {-1: [[state_id, 1.0]]}\n","\n","    def connect_states(self, states: dict[int, list[Node, dict[int, list[int,float]]]], S_num: int, A_num: int, max_connections: int) -> dict[int, list[Node, dict[int, list[list[int,float]]]]]:\n","        \"\"\"\n","            Connects all nodes in the graph.\n","        \"\"\"\n","        for state in states:\n","            if isinstance(states[state][0], TeleportNode):\n","                connections = self.connect_teleport(state, S_num)\n","                states[state].append(connections)\n","            elif isinstance(states[state][0], TerminalNode):\n","                connections = self.connect_terminal(state)\n","                states[state].append(connections)\n","            else:\n","                connections = self.connect_state(state, S_num, A_num, max_connections)\n","                connections = self.asign_prob_to_connections(connections)\n","                states[state].append(connections)\n","        return states\n","\n","    def create_node(self, node_type: int) -> Node:\n","        if node_type == 0:\n","          return RegularNode(-1)\n","        elif node_type == 1:\n","          return PenaltyNode(-10)\n","        elif node_type == 2:\n","          return TeleportNode(0)\n","        else:\n","          return TerminalNode(0)\n","\n","    def __init__(self, S_num: int, A_num: int, max_connections: int, node_specs: list[int]):\n","\n","        states = {}\n","        node_specs_copy = node_specs\n","        node_type = 0\n","        for i in range(S_num): #creates a skeleton of the graph assigning state ids (keys) as integers, and the type of each node\n","            while node_specs_copy[node_type] == 0: #if?\n","                node_type += 1\n","            states[i] = [self.create_node(node_type)]\n","            node_specs_copy[node_type] -= 1\n","\n","        states = self.connect_states(states, S_num, A_num, max_connections) #connects the nodes\n","        self.states = states\n","        self.S_num = S_num\n","        self.A_num = A_num\n","        self.max_connections = max_connections\n","        self.node_specs = node_specs\n","\n","    def do_action(self, state_id: int, action: int) -> int:\n","        \"\"\"\n","            Returns the next state by doing the action (action) from the state (state_id) respecting the probabilities assigned to connections.\n","        \"\"\"\n","\n","        possible_next_states = self.states[state_id][1][action]\n","        probabilities = [s[1] for s in possible_next_states]\n","\n","        choices = np.arange(len(probabilities))\n","        chosen_value = np.random.choice(choices, p=probabilities)\n","\n","        for state in self.states:\n","            if state == possible_next_states[chosen_value][0]:\n","                return state\n","            else:\n","              pass\n","\n","    def __call__(self, state_id: int, action: int) -> tuple[int, float, bool]:\n","        \"\"\"\n","            Applies the action (action) from the state (state_id) and gets the id of the next state,\n","            the reward of that action (specificaly that connection) and if the next state is terminal\n","        \"\"\"\n","        next_state = self.do_action(state_id, action)\n","        reward = self.states[next_state][0].get_reward()\n","        terminal = self.states[next_state][0].is_terminal()\n","        return next_state, reward, terminal\n","\n","    def get_actions(self, state_id) -> list[int]:\n","        \"\"\"\n","            Lists the actions that are possible to take in the state (state_id)\n","        \"\"\"\n","        return [k for k,_ in self.states[state_id][1].items()]\n","\n","    def get_states(self) -> list[int]:\n","        \"\"\"\n","            Lists the nodes (states) in the graph.\n","        \"\"\"\n","        return [v for v in self.states.keys()]\n","\n","    def is_terminal(self, state_id: int) -> bool:\n","        return self.states[state_id][0].is_terminal()\n","\n","    def get_reward(self, state_id:int) -> int:\n","        return self.states[state_id][0].get_reward()\n","\n","    def show_graph(self):\n","        \"\"\"\n","          Displays the graph\n","        \"\"\"\n","        for state in self.states:\n","          print(state, (self.states[state][0].__class__.__name__, self.states[state][1]))"],"metadata":{"id":"_RAzYf4KmH8R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defining the Maze Environment. Value and Action Value iteration. Policy iteration"],"metadata":{"id":"9Mu0_eGCFNnC"}},{"cell_type":"code","source":["class MazeEnvironment:\n","\n","    def init_stochastic_policy(self) -> dict[int, dict[int, float]]: #ako zelim da politika bude stohasticka\n","        policy = {}\n","        for state in self.maze.states:\n","            actions = self.maze.get_actions(state)\n","\n","            probabilities = [round(1.0/len(actions), 2) for i in range(len(actions))]\n","\n","            policy[state] = {actions[i]: probabilities[i] for i in range(len(actions))}\n","\n","        return policy\n","\n","    def init_deterministic_policy(self) -> dict[int, int]:\n","        policy = {}\n","        for state in self.maze.states:\n","            actions = self.maze.get_actions(state)\n","            policy[state] = actions[random.randint(0, len(actions)-1)]\n","\n","        return policy\n","\n","    def init_values(self) -> dict[int, float]:\n","        \"\"\"Randomly initialize values of states of the given environment.\"\"\"\n","        values = {s: -10*random.random() for s in self.maze.get_states()}\n","\n","        for state in values:\n","            if self.maze.is_terminal(state):\n","                values[state] = 0\n","\n","        return values\n","\n","    def init_action_values(self) -> dict[tuple[int, int], float]:\n","        \"\"\"Randomly initialize action values of states of the given environment.\"\"\"\n","        action_values = {}\n","        for state in self.maze.get_states():\n","            if self.maze.is_terminal(state):\n","                action_values[(state, -1)] = 0\n","            else:\n","                for action in self.maze.get_actions(state):\n","                    action_values[(state, action)] = -10*random.random()\n","\n","        return action_values\n","\n","    def __init__(self, maze: MazeGraph, policy: str):\n","        self.maze = maze\n","        if policy == 'deterministic':\n","            self.policy = self.init_deterministic_policy()\n","        elif policy == 'stochastic':\n","            self.policy = self.init_stochastic_policy()\n","        self.values = self.init_values()\n","        self.action_values = self.init_action_values()\n","\n","     ##### VALUE ITERATION#####\n","\n","    def update_state_value(self, state_id: int, values: dict[int, float], gamma: float) -> float:\n","        \"\"\"Update value of the given state.\n","\n","            state_id : id of the state for which the value is updated\n","            values : Values of other states.\n","            gamma : discount factor.\n","        \"\"\"\n","        temp = []\n","        for action in self.maze.get_actions(state_id):\n","            sum = 0\n","            for next_state, probability in self.maze.states[state_id][1][action]:\n","                reward = self.maze.get_reward(next_state)\n","                sum += (probability*(reward + gamma*values[next_state]))\n","            temp.append(sum)\n","        return max(temp)\n","\n","    def async_update_all_values(self, values: dict[int, float], gamma) -> list[float]:\n","        \"\"\"Update values of all states\n","            values : Values of other states.\n","            gamma : discount factor.\n","        \"\"\"\n","        new_values = copy(values)\n","        for state in self.maze.get_states():\n","            if not self.maze.is_terminal(state):\n","                new_values[state] = self.update_state_value(state, new_values, gamma)\n","        return new_values\n","\n","    def value_iteration(self, gamma, epsilon, v0, maxiter=1000) -> tuple[list[float], int]:\n","        values = v0\n","        for k in range(maxiter):\n","            new_values = self.async_update_all_values(values, gamma)\n","            error = max([abs(new_values[state] - values[state]) for state in values])\n","            if error < epsilon:\n","               return new_values, k\n","            values = new_values\n","        return values, k\n","\n","    def greedy_action_by_value(self, state, values, gamma) -> int:\n","        final = {}\n","        for action in self.maze.get_actions(state):\n","            temp = []\n","            for next_state, _ in self.maze.states[state][1][action]:\n","                reward = self.maze.get_reward(next_state)\n","                temp.append(reward + gamma*values[next_state])\n","            final[action] = max(temp)\n","        return max(final, key = lambda k: final[k])\n","\n","    def optimal_policy_value_iteration(self, values, gamma):\n","        return {state: self.greedy_action_by_value(state, values, gamma) for state in self.maze.get_states() if not self.maze.is_terminal(state)}\n","\n","    ##### ACTION VALUE (Q) ITERATION#####\n","\n","    def update_action_value(self, state_id: int, action: int, action_values: dict[tuple[int, int], float], gamma: float) -> float:\n","        \"\"\"Update action value of the given state.\n","            state_id : The id of the state (node).\n","            action : action to take from the given state.\n","            action_values : action values for all states\n","            gamma : discount factor.\n","        \"\"\"\n","        sum = 0\n","        for next_state, probability in self.maze.states[state_id][1][action]:\n","            reward = self.maze.get_reward(next_state)\n","            temp = []\n","            for next_action in self.maze.get_actions(next_state):\n","                temp.append(action_values[(next_state, next_action)])\n","            sum += (probability*(reward + gamma*max(temp)))\n","        return sum\n","\n","    def async_update_all_action_values(self, action_values: dict[tuple[int, int], float], gamma: float) -> list[float]:\n","        \"\"\"Update action values of all states.\n","            action_values : action values for all states\n","            gamma : discount factor.\n","        \"\"\"\n","        new_action_values = copy(action_values)\n","        for state in self.maze.get_states():\n","            if not self.maze.is_terminal(state):\n","                for action in self.maze.get_actions(state):\n","                    new_action_values[(state, action)] = self.update_action_value(state, action, new_action_values, gamma)\n","        return new_action_values\n","\n","    def action_value_iteration(self, gamma, epsilon, a0, maxiter=1000) -> tuple[list[float], int]:\n","        action_values = a0\n","        for k in range(maxiter):\n","            new_action_values = self.async_update_all_action_values(action_values, gamma)\n","            error = max([abs(new_action_values[state] - action_values[state]) for state in action_values])\n","            if error < epsilon:\n","               return new_action_values, k\n","            action_values = new_action_values\n","        return action_values, k\n","\n","    def greedy_action_by_action_value(self, state, action_values, gamma) -> int:\n","        temp = {}\n","        for action in self.maze.get_actions(state):\n","            temp[action] = action_values[(state, action)]\n","\n","        return max(temp, key = lambda k: temp[k])\n","\n","    def optimal_policy_action_value_iteration(self, action_values: dict[tuple[int, int], float], gamma: float) -> dict[int, int]:\n","        return {state: self.greedy_action_by_action_value(state, action_values, gamma) for state in self.maze.get_states() if not self.maze.is_terminal(state)}\n","\n","    ##### POLICY ITERATION USING STATE VALUES (V)#####\n","\n","    def derive_new_deterministic_policy_using_state_values(self, values: dict[int, float], gamma: float) -> dict[int, int]:\n","        new_policy = {}\n","        for state in self.maze.get_states():\n","            temp = {}\n","            for action in self.maze.get_actions(state):\n","                sum = 0\n","                for next_state, probability in self.maze.states[state][1][action]:\n","                    reward = self.maze.get_reward(next_state)\n","                    sum += (probability*(reward + gamma*values[next_state]))\n","                temp[action] = sum\n","            new_policy[state] = max(temp, key = lambda k: temp[k])\n","\n","        return new_policy\n","\n","    def update_state_values_using_given_policy(self, policy: dict[int, int], values: dict[int, float], gamma: float) -> dict[int, float]:\n","        new_values = {}\n","        for state in policy:\n","            sum = 0\n","            for next_state, probability in self.maze.states[state][1][policy[state]]:\n","                reward = self.maze.get_reward(next_state)\n","                sum += (probability*(reward + gamma*values[next_state]))\n","            new_values[state] = sum\n","\n","        return new_values\n","\n","    def derive_state_values_using_given_policy(self, policy: dict[int, int], v0: dict[int, float], gamma: float, epsilon: float, max_iterations: int) -> dict[int, float]:\n","        values = v0\n","        for k in range(max_iterations):\n","            new_values = self.update_state_values_using_given_policy(policy, values, gamma)\n","            error = max([abs(new_values[state] - values[state]) for state in values])\n","            if error < epsilon:\n","                return new_values\n","            values = new_values\n","        return values\n","\n","    def policy_iteration_using_state_values(self, gamma: float, epsilon: float, max_iterations=1000) -> tuple[dict[int, int], int]:\n","        values = self.init_values()\n","        policy = self.init_deterministic_policy()\n","\n","        count = 0 #counts how many times the policy hasn't changed\n","        iterations = 0\n","        while True:\n","            iterations += 1\n","            new_policy = self.derive_new_deterministic_policy_using_state_values(values, gamma)\n","\n","            if new_policy == policy:\n","                count += 1\n","\n","            if count == 3:\n","                return (new_policy, iterations)\n","            elif iterations == max_iterations:\n","                return (new_policy, iterations)\n","\n","            policy = new_policy\n","            values = self.derive_state_values_using_given_policy(policy, values, gamma, epsilon, max_iterations)\n","\n","      ##### POLICY ITERATION USING STATE ACTION VALUES (Q) #####\n","\n","    def derive_new_deterministic_policy_using_action_values(self, action_values: dict[tuple[int, int], float], gamma: float) -> dict[int, int]:\n","        new_policy = {}\n","        for state in self.maze.get_states():\n","            temp = {}\n","            for action in self.maze.get_actions(state):\n","                temp[action] = action_values[(state, action)]\n","            new_policy[state] = max(temp, key = lambda k: temp[k])\n","\n","        return new_policy\n","\n","    def update_action_values_using_given_policy(self, policy: dict[int, int], action_values: dict[tuple[int, int], float], gamma: float) -> dict[tuple[int, int], float]:\n","        new_action_values = {}\n","        for state in policy:\n","            for action in self.maze.get_actions(state):\n","                sum = 0\n","                for next_state, probability in self.maze.states[state][1][action]:\n","                    reward = self.maze.get_reward(next_state)\n","                    sum += (probability*(reward + gamma*action_values[(next_state, policy[next_state])]))\n","                new_action_values[(state, action)] = sum\n","\n","        return new_action_values\n","\n","    def derive_action_values_using_given_policy(self, policy: dict[int, int], a0: dict[tuple[int, int], float], gamma: float, epsilon: float, max_iterations: int) -> dict[int, float]:\n","        action_values = a0\n","        for k in range(max_iterations):\n","            new_action_values = self.update_action_values_using_given_policy(policy, action_values, gamma)\n","            error = max([abs(new_action_values[(state, action)] - action_values[(state, action)]) for state, action in action_values])\n","            if error < epsilon:\n","                return new_action_values\n","            action_values = new_action_values\n","        return action_values\n","\n","    def policy_iteration_using_action_values(self, gamma: float, epsilon: float, max_iterations=1000) -> tuple[dict[int, int], int]:\n","        action_values = self.init_action_values()\n","        policy = self.init_deterministic_policy()\n","\n","        count = 0\n","        iterations = 0\n","        while True:\n","            iterations += 1\n","            new_policy = self.derive_new_deterministic_policy_using_action_values(action_values, gamma)\n","\n","            if new_policy == policy:\n","                count += 1\n","\n","            if count == 3:\n","                return (new_policy, iterations)\n","            elif iterations == max_iterations:\n","                return (new_policy, iterations)\n","\n","            policy = new_policy\n","            action_values = self.derive_action_values_using_given_policy(policy, action_values, gamma, epsilon, max_iterations)\n"],"metadata":{"id":"Zvi8JC4o0VfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing\n","\n","Here we have an implementation of a class that will test the code written thus far. Hiperparameters can be set here."],"metadata":{"id":"lp7FUkcfPaTr"}},{"cell_type":"code","source":["class Test:\n","    def __init__(self):\n","\n","          self.test_specs = {'S_NUM' : 5,\n","                            'A_NUM' : 3,\n","                            'MAX_CONNECTIONS' : 15,\n","                            'NODE_SPECS' : [2, 1, 1, 1],\n","                            'GAMMA' : 1,\n","                            'EPSILON' : 0.0000001,\n","                            'MAX_ITERATIONS' : 1000,\n","                            'POLICY' : 'deterministic'}\n","\n","          try:\n","              self.maze = MazeGraph(self.test_specs['S_NUM'], self.test_specs['A_NUM'], self.test_specs['MAX_CONNECTIONS'], self.test_specs['NODE_SPECS'])\n","          except ZeroDivisionError:\n","              print(\"Oops :'(. Try again\")\n","          self.environment = MazeEnvironment(self.maze, self.test_specs['POLICY'])\n","\n","    def show_graph(self):\n","        self.maze.show_graph()\n","\n","    def value_iteration(self):\n","        self.environment.values, k = self.environment.value_iteration(self.test_specs['GAMMA'], self.test_specs['EPSILON'], self.environment.values)\n","        print(self.environment.values, k)\n","\n","    def optimal_policy_using_state_value_iteration(self):\n","        print(self.environment.optimal_policy_value_iteration(self.environment.values, 1))\n","\n","    def action_value_iteration(self):\n","        self.environment.action_values, k = self.environment.action_value_iteration(self.test_specs['GAMMA'], self.test_specs['EPSILON'], self.environment.action_values)\n","        print(self.environment.action_values, k)\n","\n","    def optimal_policy_using_action_value_iteration(self):\n","        print(self.environment.optimal_policy_action_value_iteration(self.environment.action_values, self.test_specs['GAMMA']))\n","\n","    def policy_iteration_using_state_values(self):\n","        opt_pol, i = self.environment.policy_iteration_using_state_values(self.test_specs['GAMMA'], self.test_specs['EPSILON'], self.test_specs['MAX_ITERATIONS'])\n","        print(opt_pol, i)\n","\n","    def policy_iteration_using_action_values(self):\n","        opt_pol, i = self.environment.policy_iteration_using_action_values(self.test_specs['GAMMA'], self.test_specs['EPSILON'], self.test_specs['MAX_ITERATIONS'])\n","        print(opt_pol, i)\n"],"metadata":{"id":"Gmd-4U-rPZqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following code block will create a graph and print it in form of a dictionary. Due to the nature of randomness of this implementation it is possible, depending on the number of nodes, actions and connections, to get isolated nodes, graph that never reaches terminal state... In order to prevent that from happening it would've been nice for us to have implemented a method that will validate the graph, but that would take implementing DFS, Topological Sorting or some other algorithm concerning graph manipulation. We decided that that would have been an overkill so, please feel free to generate a new graph if such a scenario occurs :).\n","Also, sometimes (very rarely), a zero division error occurs when generating random probabilities. This is handled by an exception, so in that case try again.\n","\n","Example:\n","\n","0 ('RegularNode', {0: [[4, 1.0]], 1: [[1, 0.42], [2, 0.19], [4, 0.39]], 2: [[0, 0.49], [1, 0.51]]})\n","\n","Here the 0 at the begining is the state ID, the keys of the dictionary are actions, and the pair for example [2, 0.19] means the ID of the next state is 2, and the probability of that actualy being the next state if the action 1 is chosen is 0.19."],"metadata":{"id":"o2hrymWXXLpp"}},{"cell_type":"code","source":["test = Test()\n","test.show_graph()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZsFyL_UrTSUH","executionInfo":{"status":"ok","timestamp":1701619667939,"user_tz":-60,"elapsed":2,"user":{"displayName":"Aleksa Pulai","userId":"02720890439751117842"}},"outputId":"ed105b65-6a2d-4de2-a4e4-a748cf89c1be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 ('RegularNode', {0: [[4, 1.0]], 1: [[1, 0.42], [2, 0.19], [4, 0.39]], 2: [[0, 0.49], [1, 0.51]]})\n","1 ('RegularNode', {0: [[0, 0.17], [1, 0.15], [3, 0.22], [4, 0.46]], 1: [[2, 0.88], [4, 0.12]]})\n","2 ('PenaltyNode', {0: [[4, 1.0]], 1: [[1, 0.26], [2, 0.45], [4, 0.29]], 2: [[0, 0.35], [1, 0.32], [2, 0.2], [3, 0.13]]})\n","3 ('TeleportNode', {0: [[2, 1.0]]})\n","4 ('TerminalNode', {-1: [[4, 1.0]]})\n"]}]},{"cell_type":"code","source":["test.value_iteration()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrGFrtbBWp5t","executionInfo":{"status":"ok","timestamp":1701619669915,"user_tz":-60,"elapsed":245,"user":{"displayName":"Aleksa Pulai","userId":"02720890439751117842"}},"outputId":"4de80ee4-68b6-4f17-fbcb-04a00e02973f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0.0, 1: -2.964705873541097, 2: 0.0, 3: -10.0, 4: 0} 9\n"]}]},{"cell_type":"code","source":["test.optimal_policy_using_state_value_iteration()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAlXuobBXBYK","executionInfo":{"status":"ok","timestamp":1701619671441,"user_tz":-60,"elapsed":243,"user":{"displayName":"Aleksa Pulai","userId":"02720890439751117842"}},"outputId":"e70ae9b3-cc21-44be-ec89-56a10820a9ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0, 1: 0, 2: 0, 3: 0}\n"]}]},{"cell_type":"code","source":["test.action_value_iteration()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jOj82EyWs1M","executionInfo":{"status":"ok","timestamp":1701619672316,"user_tz":-60,"elapsed":2,"user":{"displayName":"Aleksa Pulai","userId":"02720890439751117842"}},"outputId":"8d4c1d2a-e276-42db-be93-4cffa6719845"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{(0, 0): 0.0, (0, 1): -3.5651764658025016, (0, 2): -2.511999994188752, (1, 0): -2.9647058806437507, (1, 1): -8.8, (2, 0): 0.0, (2, 1): -5.5308235289673755, (2, 2): -4.918705881806, (3, 0): -10.0, (4, -1): 0} 10\n"]}]},{"cell_type":"code","source":["test.optimal_policy_using_action_value_iteration()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ka-KTacaXEqN","executionInfo":{"status":"ok","timestamp":1701619678939,"user_tz":-60,"elapsed":232,"user":{"displayName":"Aleksa Pulai","userId":"02720890439751117842"}},"outputId":"76d87402-46b4-4585-d624-6b8bd8ffd117"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0, 1: 0, 2: 0, 3: 0}\n"]}]},{"cell_type":"code","source":["test.policy_iteration_using_state_values()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BVnrlQHWuqW","executionInfo":{"status":"ok","timestamp":1701619679911,"user_tz":-60,"elapsed":2,"user":{"displayName":"Aleksa Pulai","userId":"02720890439751117842"}},"outputId":"18502282-286e-4518-ae2b-747fd313dc65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0, 1: 0, 2: 0, 3: 0, 4: -1} 4\n"]}]},{"cell_type":"code","source":["test.policy_iteration_using_action_values()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1uSkv0pWyMo","executionInfo":{"status":"ok","timestamp":1701619680885,"user_tz":-60,"elapsed":2,"user":{"displayName":"Aleksa Pulai","userId":"02720890439751117842"}},"outputId":"2d89d097-baa1-48a8-fd37-31de6da27887"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0, 1: 0, 2: 0, 3: 0, 4: -1} 5\n"]}]}]}